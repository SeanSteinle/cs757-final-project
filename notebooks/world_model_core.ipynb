{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98bd1649-aa3f-4c0c-a7a2-e9a7d07e76c8",
   "metadata": {},
   "source": [
    "# World Model Core\n",
    "*Sean Steinle, Kiya Aminfar*\n",
    "\n",
    "This notebook walks through the core aspects of world models, developing crucial pieces of code sequentially. Not that this code isn't meant for scale -- instead, this is for a demonstration of how we developed the code that we did.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Collecting Rollout Data](#Collecting-Rollout-Data)\n",
    "2. [Training the VAE](#Training-the-VAE)\n",
    "3. [Training the MDN-RNN](#Training-the-MDN-RNN)\n",
    "    - [Prepping Rollout Data for the MDN-RNN](#Prepping-Rollout-Data-for-the-MDN-RNN)\n",
    "    - [Core Training](#Core-Training)\n",
    "4. [Training the Controller](#Training-the-Controller)\n",
    "5. [Early Results](#Early-Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21544f3f-d6e2-4fda-8b1d-4e6b51e934bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7847949-423a-47be-a72d-62322c9143ee",
   "metadata": {},
   "source": [
    "## Collecting Rollout Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fe9254-55f2-4148-84fc-aa3d9452e7af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's begin by creating an instance of our humanoid environment and checking out what basic observations look like.\n",
    "env = gym.make('Humanoid-v5', render_mode=\"rgb_array\")\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47c4e48-4af5-4ca9-be13-59301b800e17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((348,),\n",
       " array([ 1.40752215e+00,  1.00663365e+00,  6.02556701e-03,  1.16719831e-03,\n",
       "        -4.62792461e-03,  5.14522305e-03, -9.58258864e-03,  3.63368217e-03,\n",
       "         1.28224640e-04,  9.20090990e-03,  8.16243137e-03,  6.34724809e-03,\n",
       "        -2.19157998e-03, -4.84501161e-03,  5.03633911e-03, -3.15465082e-03,\n",
       "         2.63887772e-03, -6.97352955e-03,  9.36419674e-03, -8.20846185e-05,\n",
       "        -1.74765736e-03,  6.09327069e-03, -5.16949615e-03, -4.27053733e-03,\n",
       "         1.70261180e-03,  6.54108492e-03,  1.32864334e-03, -1.15680755e-03,\n",
       "        -3.40851327e-03,  6.35224940e-03, -3.04810121e-03, -2.93511876e-04,\n",
       "         7.56070784e-03,  8.48507331e-03, -6.35783697e-03,  3.95974482e-03,\n",
       "         2.30324114e-03,  7.89472002e-03,  8.49384980e-03,  5.69242465e-04,\n",
       "         5.84585705e-03,  4.37064015e-03, -4.43530877e-03,  8.93418639e-03,\n",
       "        -1.32483179e-04,  2.30367321e+00,  2.28714761e+00,  4.46661737e-02,\n",
       "        -1.23613986e-03,  7.63484451e-02,  3.07654488e-02, -1.63893363e-01,\n",
       "        -6.00825989e-02,  4.35592900e+00,  8.90746237e+00,  9.53585139e-02,\n",
       "         9.07477180e-02,  1.13924058e-02, -2.09308008e-04,  1.13903735e-02,\n",
       "         1.31159019e-03, -5.85936029e-02, -7.12226511e-03,  4.39725286e-01,\n",
       "         2.26194671e+00,  5.79326948e-02,  4.27688992e-02,  6.51426872e-02,\n",
       "        -3.26634072e-04,  8.48264379e-03, -2.94409095e-04, -2.91328037e-01,\n",
       "        -4.87043300e-03,  1.92684463e-01,  6.61619413e+00,  2.73063035e-01,\n",
       "         2.34677121e-01,  5.14865517e-02, -9.97121269e-03, -1.91045792e-02,\n",
       "        -7.70338812e-02, -1.07529191e-01, -4.39700436e-01, -8.64901769e-01,\n",
       "         4.75175093e+00,  9.30217956e-01,  9.12866056e-01,  2.25066704e-02,\n",
       "        -4.23786235e-03, -2.89131762e-02, -1.26484217e-01, -5.18892312e-02,\n",
       "        -2.24731036e-01, -1.55661525e+00,  2.75569617e+00,  1.04856041e+00,\n",
       "         1.03815164e+00,  1.53039566e-02, -2.23485550e-03, -2.17955969e-02,\n",
       "        -1.05994156e-01, -2.84973415e-02, -1.38585407e-01, -1.35156464e+00,\n",
       "         1.76714587e+00,  2.72739325e-01,  2.29437867e-01,  5.58405153e-02,\n",
       "         9.89007802e-03, -1.74037886e-02,  8.24297349e-02, -1.01044932e-01,\n",
       "         4.64713118e-01, -8.50830933e-01,  4.75175093e+00,  9.30517400e-01,\n",
       "         9.04060903e-01,  3.14054004e-02,  4.89453685e-03, -2.74370009e-02,\n",
       "         1.54761738e-01, -4.91884836e-02,  2.74416294e-01, -1.54889163e+00,\n",
       "         2.75569617e+00,  1.04893900e+00,  1.03062299e+00,  2.32449403e-02,\n",
       "         2.99219512e-03, -2.21109879e-02,  1.38867713e-01, -2.90158024e-02,\n",
       "         1.82233293e-01, -1.34662279e+00,  1.76714587e+00,  4.28783421e-01,\n",
       "         3.25094713e-01,  1.26635542e-01,  3.07607183e-02, -3.86881324e-02,\n",
       "         1.78770789e-01,  1.01187156e-01, -4.26794937e-01,  7.15136942e-01,\n",
       "         1.66108048e+00,  3.22985847e-01,  3.35888023e-01,  1.74867700e-01,\n",
       "         7.82933710e-02, -1.49870808e-01,  1.30406816e-01,  3.31089060e-01,\n",
       "        -3.06694532e-01,  5.40084586e-01,  1.22954019e+00,  4.26607498e-01,\n",
       "         3.35938410e-01,  1.16581128e-01, -3.17735280e-02, -4.34412524e-02,\n",
       "        -1.71673562e-01,  1.10480509e-01,  4.03665154e-01,  7.26299143e-01,\n",
       "         1.66108048e+00,  3.20507833e-01,  3.47764286e-01,  1.67827094e-01,\n",
       "        -7.36175305e-02, -1.55407301e-01, -1.22316015e-01,  3.38223259e-01,\n",
       "         2.83553787e-01,  5.49567600e-01,  1.22954019e+00,  6.55040642e-03,\n",
       "         1.28230245e-03, -1.15635847e-03, -5.74476902e-03, -1.31276388e-03,\n",
       "         1.72378950e-03,  6.58213336e-03,  7.67475992e-03, -4.48870239e-03,\n",
       "        -7.38985312e-03, -1.39330694e-03,  1.55362051e-03,  3.53441493e-03,\n",
       "         7.68770038e-03, -4.53524181e-03, -7.39141723e-03, -1.78888603e-03,\n",
       "         1.54605677e-03,  3.08093696e-03,  1.60545528e-02,  3.15129757e-03,\n",
       "        -8.06010050e-03, -1.60027692e-03,  1.30130453e-03,  3.04791216e-03,\n",
       "         2.24115331e-02,  3.25030051e-03, -5.55801390e-03, -1.58514343e-03,\n",
       "         1.16421399e-03,  3.04791216e-03,  2.24115331e-02,  3.25030051e-03,\n",
       "        -5.55801390e-03, -1.58514343e-03,  1.16421399e-03, -3.96371360e-04,\n",
       "         1.56387882e-02, -6.75856015e-03, -7.54168774e-03, -1.80467838e-03,\n",
       "         1.75525532e-03, -3.88976377e-04,  7.14627244e-03, -6.90891474e-03,\n",
       "        -1.08861238e-02, -1.81044452e-03,  1.91645428e-03, -3.88976377e-04,\n",
       "         7.14627244e-03, -6.90891474e-03, -1.08861238e-02, -1.81044452e-03,\n",
       "         1.91645428e-03,  6.99793456e-03, -2.68395134e-03,  3.15254290e-03,\n",
       "        -4.47569477e-03, -1.00864947e-03,  1.87191275e-03,  6.98305526e-03,\n",
       "        -5.81776664e-03,  6.19909432e-03, -4.53390092e-03, -1.49724259e-03,\n",
       "         1.36904034e-03,  3.01462905e-03,  9.38918009e-03,  3.45541599e-03,\n",
       "        -9.17972502e-03, -3.07291875e-03,  2.18437587e-03,  3.01571007e-03,\n",
       "         9.48173303e-03,  3.55020300e-03, -9.17851208e-03, -3.08842483e-03,\n",
       "         2.19950266e-03,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2035ecfe-44e4-4ab0-b4bb-7213f9d50c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_position': np.float64(0.0025825735263227626),\n",
       " 'y_position': np.float64(-0.0014417381873546332),\n",
       " 'tendon_length': array([-0.00819099, -0.00181518]),\n",
       " 'tendon_velocity': array([ 0.00059913, -0.01484291]),\n",
       " 'distance_from_origin': np.float64(0.0029577516832452)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de32bf7-40c6-40e3-bbd3-b1131e598b18",
   "metadata": {},
   "source": [
    "As we can see, the humanoid environment gives us a TON of observations! We get dozens of variables representing various positions and velocities of body parts, the center of mass, and a lot of other variables I hardly understand. For an exhaustive list, see the [doc](https://gymnasium.farama.org/environments/mujoco/humanoid/#observation-space). The fact that there are so many variables here is what makes learning latent observations so obvious!\n",
    "\n",
    "We also get some nice summary stats in info, but we aren't going to include them in our scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00bc4518-6772-473b-8b89-5984a755ac24",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ']' (2640406133.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[83], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    episode_observations, episode_rewards, episode_actions = [], [], []]\u001b[0m\n\u001b[0m                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ']'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def collect_rollout_data(env_name: str, out_dir: str, n_timesteps: int=10000, print_n_episodes: int=1000):\n",
    "    \"\"\"Simulates `n_timesteps` in the `env_name` environment, saving observations, rewards, and actions to a triplet of .npy files at `out_dir`.\"\"\"\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    obs, info = env.reset()\n",
    "    observations, rewards, actions = [], [] , []\n",
    "    episode_count = 0\n",
    "\n",
    "    for timestep in range(n_timesteps):  # Run for n_timesteps or until the episode ends\n",
    "        action = env.action_space.sample() #select random action\n",
    "        obs, reward, terminated, truncated, info = env.step(action) #execute and get results\n",
    "        observations.append(obs) #save observation\n",
    "        rewards.append(reward) #save reward\n",
    "        actions.append(action) #save action\n",
    "        if terminated or truncated: #check for game over, if so reset env\n",
    "            episode_count+=1\n",
    "            total_observations.append(episode_observations)\n",
    "            total_rewards.append(episode_rewards)\n",
    "            total_actions.append(episode_actions)\n",
    "            if episode_count % print_n_episodes == 0: print(f\"finished {episode_count} episodes\") #provide update on training\n",
    "            observation, info = env.reset()\n",
    "        env.close()\n",
    "    np_obs, np_rewards, np_actions = np.array(observations), np.array(rewards), np.array(actions)\n",
    "    print(f\"observations has shape: {np_obs.shape}\\trewards has shape: {np_rewards.shape}\\tactions has shape: {np_actions.shape}\")\n",
    "    np.save(f'{out_dir}/{env_name}_{n_timesteps}_rollout_observations.npy', np_obs) #load with: new_obs = np.load(\"../data/processed/Humanoid-v5_10000_rollout_observations.npy\")\n",
    "    np.save(f'{out_dir}/{env_name}_{n_timesteps}_rollout_rewards.npy', np_rewards)\n",
    "    np.save(f'{out_dir}/{env_name}_{n_timesteps}_rollout_actions.npy', np_actions)\n",
    "    return np_obs, np_rewards, np_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c936e8b4-1012-48af-9f9d-a1c0fb0da2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 100 episodes\n",
      "finished 200 episodes\n",
      "finished 300 episodes\n",
      "finished 400 episodes\n",
      "observations has shape: (10000, 348)\trewards has shape: (10000,)\tactions has shape: (10000, 17)\n"
     ]
    }
   ],
   "source": [
    "humanoid_obs, humanoid_rewards, humanoid_actions, humanoid_done = collect_rollout_data('Humanoid-v5', \"../data/processed\", 10000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bea542a-9109-4bd2-b80f-8e4775dde9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.89014007, 4.92889786, 4.94746227, ..., 4.52836352, 4.57897921,\n",
       "       4.56725108])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We see that the number of samples per episode changes, but the observation space's dimensionality never does!\n",
    "humanoid_obs[0].shape, humanoid_obs[1].shape, humanoid_obs[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26d613d5-e3a6-43f6-abf0-c3cfe81bd712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1344295 , -0.33341795,  0.2567148 , ..., -0.26523423,\n",
       "        -0.22723526, -0.34251404],\n",
       "       [-0.2680746 , -0.05566841, -0.06947935, ..., -0.19162744,\n",
       "        -0.23866333,  0.33647585],\n",
       "       [-0.1664092 ,  0.0057418 , -0.32716182, ...,  0.06560624,\n",
       "         0.14931448,  0.23692013],\n",
       "       ...,\n",
       "       [-0.3987542 ,  0.35181522,  0.2205222 , ..., -0.27054495,\n",
       "         0.26330063, -0.19415343],\n",
       "       [-0.26650003, -0.35020053, -0.03938405, ..., -0.18459655,\n",
       "         0.04306056,  0.2507781 ],\n",
       "       [-0.25835124,  0.19055103, -0.2883535 , ...,  0.04691168,\n",
       "        -0.00619155, -0.23913088]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Same holds for actions! We can even see the dimensionality of actions and observations are similar based on episode length\n",
    "humanoid_actions[0].shape, humanoid_actions[1].shape, humanoid_actions[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0536b8fe-b7c4-4b15-9ec7-968f2c8b6845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False, False, False, ..., False, False, False]), np.int64(413))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humanoid_done, humanoid_done.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60900362-cf6f-4514-9708-4fad349d3e9f",
   "metadata": {},
   "source": [
    "## Training the VAE\n",
    "\n",
    "Now that we have an easy function for gathering experiences in the environment, we need to train the VAE module of our world model which will compress the observation space into latent space with fewer dimensions.\n",
    "\n",
    "Note that the original World Model implementation worked with tensorflow 1.18.0. This is incredibly outdated (worked with Python 3.5), so let's get a newer version (tensorflow 2.19.0). Additionally, we need to change the structure of the VAE from working with images to working with a vector of observation data! Luckily, ChatGPT is very good at updating code (or it will be very obvious if it is not!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1389404-530a-43a4-862c-0403f544239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 11:48:41.087368: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-09 11:48:41.107521: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746805721.126734   10102 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746805721.132519   10102 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746805721.147023   10102 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746805721.147049   10102 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746805721.147051   10102 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746805721.147053   10102 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-09 11:48:41.153875: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, saving\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "class MLPVAE(Model):\n",
    "    def __init__(self, input_dim=348, z_size=32, kl_tolerance=0.5):\n",
    "        super(MLPVAE, self).__init__()\n",
    "        self.z_size = z_size\n",
    "        self.kl_tolerance = kl_tolerance\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(input_dim,)),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(2 * z_size),  # output both mu and logvar\n",
    "        ])\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(z_size,)),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(input_dim, activation='linear'),  # output same shape as input\n",
    "        ])\n",
    "\n",
    "    def sample_z(self, mu, logvar):\n",
    "        eps = tf.random.normal(shape=tf.shape(mu))\n",
    "        sigma = tf.exp(0.5 * logvar)\n",
    "        return mu + sigma * eps\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = tf.split(h, num_or_size_splits=2, axis=1)\n",
    "        logvar = tf.clip_by_value(logvar, -10.0, 10.0)  # helps with exploding values\n",
    "        return mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def call(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.sample_z(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "    def compute_loss(self, x):\n",
    "        x_recon, mu, logvar = self(x)\n",
    "        recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_recon), axis=1))\n",
    "        kl_loss = -0.5 * tf.reduce_sum(1 + logvar - tf.square(mu) - tf.exp(logvar), axis=1)\n",
    "        kl_loss = tf.maximum(kl_loss, self.kl_tolerance * self.z_size)\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        total_loss = recon_loss + kl_loss\n",
    "        return total_loss, recon_loss, kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d97e9ff-74f4-4a65-b397-11e381a66fb6",
   "metadata": {},
   "source": [
    "Let's also write a training function for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ac2fe07-2e6d-4246-a75a-348182aac96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(x_train, batch_size=64, shuffle_buffer=10000):\n",
    "    # Assuming x_train is a NumPy array of shape [n_samples, 348]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(x_train.astype(np.float32))\n",
    "    dataset = dataset.shuffle(shuffle_buffer).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def train_vae(model, dataset, epochs=10, learning_rate=1e-4):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        total_batches = 0\n",
    "        for x_batch in dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, recon_loss, kl_loss = model.compute_loss(x_batch)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            total_loss += loss.numpy()\n",
    "            total_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / total_batches\n",
    "        print(f\"Epoch {epoch+1}: avg loss = {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0227816c-8f87-4374-860d-49162c0256d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 11:48:47.993476: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/seansteinle/.pyenv/versions/cs757-final-project/lib/python3.10/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n",
      "2025-05-09 11:49:06.936035: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg loss = 302230.9688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 11:49:25.816401: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: avg loss = 163989.3906\n",
      "Epoch 3: avg loss = 75357.7734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 11:49:57.662699: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: avg loss = 59448.0898\n",
      "Epoch 5: avg loss = 53910.8359\n",
      "Epoch 6: avg loss = 49874.7539\n",
      "Epoch 7: avg loss = 45672.3164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 11:50:51.672229: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: avg loss = 39600.4180\n",
      "Epoch 9: avg loss = 33288.8203\n",
      "Epoch 10: avg loss = 28312.5820\n",
      "Epoch 11: avg loss = 24710.0449\n",
      "Epoch 12: avg loss = 22066.8398\n",
      "Epoch 13: avg loss = 19569.1250\n",
      "Epoch 14: avg loss = 17441.9258\n",
      "Epoch 15: avg loss = 15839.2998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 11:52:38.244919: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: avg loss = 14604.4170\n",
      "Epoch 17: avg loss = 13732.7695\n",
      "Epoch 18: avg loss = 13018.8848\n",
      "Epoch 19: avg loss = 12438.6729\n",
      "Epoch 20: avg loss = 11923.9502\n"
     ]
    }
   ],
   "source": [
    "# x_train should be a NumPy array of shape (n_samples, 348)\n",
    "x_train = humanoid_obs\n",
    "x_train = (x_train - np.mean(x_train, axis=0)) / (np.std(x_train, axis=0) + 1e-6)\n",
    "\n",
    "dataset = create_dataset(humanoid_obs, batch_size=64)\n",
    "vae = MLPVAE(input_dim=348, z_size=32)\n",
    "train_vae(vae, dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4012c-cd27-45fd-bbc7-82e4a1be44cd",
   "metadata": {},
   "source": [
    "Loss is going down! At first I got a tons of NAN values, but it's because I wasn't normalizing the input data and I also needed to clip the logvar values we were getting as a result of the encoding process. If you get NANs again, a lower learning rate could help too. Onto saving the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1ae0e21-af4e-4a66-b0a0-8be8d82d5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.save_weights('../models/vae/humanoid_10000_vae_model.weights.h5') #save ONLY weights -- much simpler than serializing the entire object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8abe6c45-c5ae-4854-989f-fa2695cbf459",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_vae = MLPVAE(input_dim=348, z_size=32) #instantiate new model object \n",
    "new_vae(tf.zeros((1, 348))) #invoke it to build its shape \n",
    "new_vae.load_weights('../models/vae/humanoid_10000_vae_model.weights.h5') #now load weights into empty vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d3e5358-9331-49b2-87c9-bba5693775d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MLPVAE name=mlpvae_1, built=True>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "474e51af-98f1-4306-8971-c5d3c8edceb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MLPVAE name=mlpvae, built=True>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d1e06-9bb3-4ffd-bcdd-1e77bc6db66c",
   "metadata": {},
   "source": [
    "## Training the MDN-RNN\n",
    "\n",
    "Now that we have a model which captures observations, we're theoretically ~1/3 done with the project! I say theoretically because this was probably the easiest part of the project. Now onto the meat of world models: capturing the transitions of our environment and training the MDN-RNN!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc9e9b5-64c8-4c11-8d0a-5ca8cdefa295",
   "metadata": {},
   "source": [
    "### Prepping Rollout Data for the MDN-RNN\n",
    "\n",
    "To train the MDN-RNN, we first need to enhance our basic rollout dataset with predictions of `mu` and `logvar` for each experience. Then we'll feed this information to the MDN-RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "960d5f8f-30a5-45e5-b013-48de1be22c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 348), (10000,), (10000, 17), (10000,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can use the dataset records still in memory. Note that we only need the observations for now!\n",
    "humanoid_obs.shape, humanoid_rewards.shape, humanoid_actions.shape, humanoid_done.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51554a1a-a2db-4db8-8442-2a18e2b27427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MLPVAE name=mlpvae, built=True>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can also use the VAE still in memory!\n",
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e69fb5c-cda7-4adb-ab82-c2e902e7a80d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.39590622e+00,  9.99995520e-01, -1.48260806e-03,  2.35242370e-03,\n",
       "        1.10815003e-03,  7.28172965e-03, -7.15410548e-02,  1.55540659e-02,\n",
       "       -1.52926195e-02,  1.69786602e-02,  7.06296636e-02, -1.89977570e-03,\n",
       "        1.16534514e-02, -6.50198093e-03,  5.72273184e-02, -8.02832134e-03,\n",
       "       -1.75396758e-02,  1.50975119e-02,  1.15088466e-02,  2.90521775e-02,\n",
       "       -2.29436430e-02,  1.14054975e-03, -3.53934903e-01, -1.84836038e-02,\n",
       "       -2.29504838e-01, -5.99725486e-02,  1.94204832e+00, -3.87114357e-01,\n",
       "        2.02086546e+00, -7.42386529e+00,  9.98969633e-01, -1.04443180e+00,\n",
       "        2.28379743e+00,  7.55341866e+00, -6.47223162e-02,  1.17495983e+00,\n",
       "       -8.30978728e-01,  6.81784841e+00, -2.13443194e-01, -1.98394812e+00,\n",
       "        1.65379456e+00,  4.63941103e-01,  2.70873725e+00, -1.69449907e+00,\n",
       "        3.42377348e-01,  2.29532303e+00,  2.28309398e+00,  4.81239231e-02,\n",
       "       -1.25221834e-05,  1.16678255e-01,  4.11431168e-04, -2.47469251e-01,\n",
       "       -1.54578979e-03,  4.34781573e+00,  8.90746237e+00,  9.46774842e-02,\n",
       "        9.08819556e-02,  1.21682758e-02, -1.58551956e-05,  1.40065730e-02,\n",
       "        4.65328783e-04, -7.23306732e-02, -2.30972504e-03,  4.38023155e-01,\n",
       "        2.26194671e+00,  5.73460198e-02,  4.00886022e-02,  6.30412348e-02,\n",
       "        2.88732839e-04,  7.35038316e-03, -3.44641623e-04, -2.66413672e-01,\n",
       "       -2.38755548e-04,  1.82448561e-01,  6.61619413e+00,  2.75339397e-01,\n",
       "        2.33625076e-01,  5.22820383e-02, -7.08373408e-03, -1.33422134e-02,\n",
       "       -8.04512701e-02, -7.45414618e-02, -4.51186636e-01, -8.65232375e-01,\n",
       "        4.75175093e+00,  9.35649595e-01,  9.13191945e-01,  2.68096127e-02,\n",
       "       -3.65791986e-03, -2.25402927e-02, -1.42500418e-01, -4.00089378e-02,\n",
       "       -2.51965057e-01, -1.55724830e+00,  2.75569617e+00,  1.05344965e+00,\n",
       "        1.03877202e+00,  1.93514565e-02, -2.28962484e-03, -1.89971963e-02,\n",
       "       -1.24676066e-01, -2.48297058e-02, -1.62954049e-01, -1.35204248e+00,\n",
       "        1.76714587e+00,  2.71707005e-01,  2.29179400e-01,  5.30338020e-02,\n",
       "        7.01608621e-03, -1.18794767e-02,  7.98193133e-02, -7.28744421e-02,\n",
       "        4.55229898e-01, -8.53112854e-01,  4.75175093e+00,  9.27370346e-01,\n",
       "        9.04790430e-01,  2.63135702e-02,  2.48005596e-03, -1.50390485e-02,\n",
       "        1.41173005e-01, -2.72362281e-02,  2.50943385e-01, -1.55007353e+00,\n",
       "        2.75569617e+00,  1.04606882e+00,  1.03148784e+00,  1.87816365e-02,\n",
       "        1.28446665e-03, -1.07407163e-02,  1.22865210e-01, -1.40863922e-02,\n",
       "        1.61137068e-01, -1.34742894e+00,  1.76714587e+00,  4.31808661e-01,\n",
       "        3.32599138e-01,  1.21312280e-01,  2.86814265e-02, -3.63278438e-02,\n",
       "        1.77154284e-01,  9.42752620e-02, -4.16902871e-01,  7.24873715e-01,\n",
       "        1.66108048e+00,  3.28222252e-01,  3.43311494e-01,  1.65487476e-01,\n",
       "        7.33790327e-02, -1.50289859e-01,  1.27596892e-01,  3.25049065e-01,\n",
       "       -2.93926252e-01,  5.52575387e-01,  1.22954019e+00,  4.32220605e-01,\n",
       "        3.32645820e-01,  1.21705681e-01, -2.86748835e-02, -3.61924992e-02,\n",
       "       -1.77507713e-01,  9.38484073e-02,  4.17520870e-01,  7.25061497e-01,\n",
       "        1.66108048e+00,  3.30831905e-01,  3.45795578e-01,  1.67415490e-01,\n",
       "       -7.43505308e-02, -1.51496933e-01, -1.29077153e-01,  3.26618030e-01,\n",
       "        2.96197915e-01,  5.54340431e-01,  1.22954019e+00, -6.60969708e-02,\n",
       "        1.94436891e+00, -3.93150661e-01, -1.23575256e+00, -5.84551017e-02,\n",
       "       -2.78915297e-01,  5.84192720e-03, -5.48194431e+00,  1.65011633e+00,\n",
       "        6.82060154e-01,  3.48707599e-02, -7.24167413e-03,  1.00254157e+00,\n",
       "       -5.47225658e+00,  1.72476818e+00,  6.80724976e-01,  1.65302097e-01,\n",
       "       -6.34164696e-03, -4.10289532e-01,  2.07885914e+00,  3.89881859e+00,\n",
       "        5.55992160e-01,  2.17739257e-01, -2.69530329e-01, -4.11992729e-01,\n",
       "        2.14330712e+00,  3.89856362e+00,  5.81478916e-01,  2.18408392e-01,\n",
       "       -2.70646259e-01, -4.11992729e-01,  2.14330712e+00,  3.89856362e+00,\n",
       "        5.81478916e-01,  2.18408392e-01, -2.70646259e-01, -3.42859185e-01,\n",
       "        1.33800034e+00,  2.46979446e+00,  8.20841807e-01,  1.91878904e-01,\n",
       "        3.74982224e-03, -3.46329924e-01,  1.55135489e+00,  2.46989080e+00,\n",
       "        9.04593464e-01,  1.93242298e-01,  1.61000246e-03, -3.46329924e-01,\n",
       "        1.55135489e+00,  2.46989080e+00,  9.04593464e-01,  1.93242298e-01,\n",
       "        1.61000246e-03, -1.69814184e+00, -2.38031700e-02, -1.92334595e-03,\n",
       "       -2.90778734e-01, -8.87543700e-01, -5.07808678e-01, -1.70055704e+00,\n",
       "       -3.45496716e-01,  3.31088685e-01, -2.98833121e-01, -9.41189751e-01,\n",
       "       -5.59689811e-01,  2.18067516e+00, -3.28647161e-01, -5.17527533e-01,\n",
       "       -9.09842943e-02,  1.09081215e+00, -6.02685212e-01,  2.18578580e+00,\n",
       "       -5.65722953e-01, -7.64683794e-01, -9.74378933e-02,  1.13185283e+00,\n",
       "       -6.42185458e-01,  2.40399972e+01,  9.64892656e+00, -7.51812011e+00,\n",
       "       -2.57628471e+01,  1.52273372e+01,  1.00754267e+02,  4.86051977e+01,\n",
       "        1.30453408e+01, -5.77212982e+00,  7.76097625e+01,  2.35200524e+01,\n",
       "       -8.96320567e+00, -4.89629880e+00,  5.39044887e+00,  9.48633775e+00,\n",
       "       -4.96498905e+00,  4.66279574e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humanoid_obs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf611482-45e7-49f5-9fe6-378f2665986f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#That's it! Just predict for our entire observation set.\n",
    "mu, logvar = vae.encode(humanoid_obs)\n",
    "humanoid_z = vae.sample_z(mu, logvar).numpy()\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3e0a85a-e3e4-477a-8f2a-bea624a053ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'../data/processed/Humanoid-v5_10000_rollout_z.npy', humanoid_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233f9ac-1c64-4814-b487-f814f2389808",
   "metadata": {},
   "source": [
    "## Core Training\n",
    "\n",
    "Now that we have a dataset prepared to train the MDN-RNN, let's define the model and test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ef572-b72a-4706-a4d7-cf3e3a899a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 'recipe' is missing task hierarchies!\n",
    "    #We never cared before because mastering domains was infeasible.\n",
    "\n",
    "#Minecraft paper offers a cool example because we have a very clear task hierarchy! But crafting task hierarchies in real environments are nontrivial -- @Nick.\n",
    "\n",
    "#Is foundation models as a prior good enough?\n",
    "#We don't just need information about what environments are like, we need goals! We need to know what it takes to be good at X.\n",
    "    #This is a RICH area for assurance research too. This set of goals is necessarily broad, but need to ensure they align tightly to our values and domain mastery.\n",
    "#Information about computers might help with tasks, but what are the tasks? And how do the tasks present a cohesive picture of domain mastery?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs757-final-project",
   "language": "python",
   "name": "cs757-final-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
