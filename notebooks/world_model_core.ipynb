{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98bd1649-aa3f-4c0c-a7a2-e9a7d07e76c8",
   "metadata": {},
   "source": [
    "# World Model Core\n",
    "*Sean Steinle, Kiya Aminfar*\n",
    "\n",
    "This notebook walks through the core aspects of world models, developing crucial pieces of code sequentially. Not that this code isn't meant for scale -- instead, this is for a demonstration of how we developed the code that we did.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Collecting Rollout Data](#Collecting-Rollout-Data)\n",
    "2. [Training the VAE](#Training-the-VAE)\n",
    "3. [Training the MDN-RNN](#Training-the-MDN-RNN)\n",
    "    - [Prepping Rollout Data for MDN-RNN](#Prepping-Rollout-Data-for-MDN-RNN)\n",
    "    - [Core Training](#Core-Training)\n",
    "4. [Training the Controller](#Training-the-Controller)\n",
    "5. [Early Results](#Early-Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21544f3f-d6e2-4fda-8b1d-4e6b51e934bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7847949-423a-47be-a72d-62322c9143ee",
   "metadata": {},
   "source": [
    "## Collecting Rollout Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fe9254-55f2-4148-84fc-aa3d9452e7af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's begin by creating an instance of our humanoid environment and checking out what basic observations look like.\n",
    "env = gym.make('Humanoid-v5', render_mode=\"rgb_array\")\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47c4e48-4af5-4ca9-be13-59301b800e17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((348,),\n",
       " array([ 1.40481817e+00,  1.00027820e+00, -7.58427598e-03, -5.84756869e-03,\n",
       "        -6.20780298e-03, -8.03125789e-03,  5.34441239e-04, -9.52304832e-03,\n",
       "         9.27527694e-03,  6.16362533e-03, -5.64603477e-03, -7.81711139e-04,\n",
       "        -7.90436194e-03, -9.48778159e-03, -6.75732395e-03, -2.45679302e-03,\n",
       "         9.02893471e-03, -8.95145457e-03, -2.64243844e-04,  6.57908475e-03,\n",
       "        -7.28041357e-03, -5.25994481e-03, -6.57503454e-03,  7.37232827e-04,\n",
       "         3.19486864e-03, -7.50552959e-03,  3.64205754e-03, -2.85553166e-03,\n",
       "         7.78082012e-03, -7.50665087e-03,  4.38009133e-03, -1.25463233e-03,\n",
       "        -7.41374027e-03,  8.10338273e-03,  4.99328496e-04, -2.31017587e-04,\n",
       "         7.73390741e-03, -8.60626718e-03, -4.45227518e-03,  2.46440318e-03,\n",
       "         6.74017134e-03,  2.83517671e-03, -8.46904104e-03, -6.50741013e-03,\n",
       "         7.69759353e-04,  2.30114170e+00,  2.28639280e+00,  4.69434572e-02,\n",
       "         1.55334553e-03,  1.02410907e-01, -3.88335083e-02, -2.13578796e-01,\n",
       "         7.60294903e-02,  4.35301022e+00,  8.90746237e+00,  9.50158736e-02,\n",
       "         9.06566955e-02,  1.16707347e-02,  1.37320434e-04,  1.23125248e-02,\n",
       "        -1.72485497e-03, -6.34584325e-02,  9.36210313e-03,  4.38807434e-01,\n",
       "         2.26194671e+00,  5.77499224e-02,  4.35335473e-02,  6.60550618e-02,\n",
       "        -2.56265806e-04,  8.65881664e-03,  4.81789031e-04, -3.01575773e-01,\n",
       "         7.09058798e-03,  1.89652182e-01,  6.61619413e+00,  2.71867030e-01,\n",
       "         2.28747897e-01,  5.62997598e-02, -1.05573751e-02, -1.75221846e-02,\n",
       "        -8.21520036e-02, -1.07555833e-01, -4.65402932e-01, -8.48111343e-01,\n",
       "         4.75175093e+00,  9.28084148e-01,  9.01596004e-01,  3.06772846e-02,\n",
       "        -3.59814630e-03, -1.97738645e-02, -1.53539012e-01, -3.64752173e-02,\n",
       "        -2.72672303e-01, -1.54704762e+00,  2.75569617e+00,  1.04663842e+00,\n",
       "         1.02834949e+00,  2.25175383e-02, -1.52484291e-03, -1.13720640e-02,\n",
       "        -1.37337955e-01, -1.49373347e-02, -1.80394957e-01, -1.34536022e+00,\n",
       "         1.76714587e+00,  2.74385656e-01,  2.35922250e-01,  5.00074648e-02,\n",
       "         8.11430027e-03, -1.43547923e-02,  7.67589507e-02, -8.76048762e-02,\n",
       "         4.35849643e-01, -8.70575140e-01,  4.75175093e+00,  9.32980739e-01,\n",
       "         9.15487822e-01,  2.11646993e-02,  2.00845827e-03, -1.34047132e-02,\n",
       "         1.24581573e-01, -2.48970895e-02,  2.21010541e-01, -1.55959999e+00,\n",
       "         2.75569617e+00,  1.05093433e+00,  1.04054566e+00,  1.44322289e-02,\n",
       "         5.93008424e-04, -5.91383152e-03,  1.03938451e-01, -7.72170108e-03,\n",
       "         1.35712633e-01, -1.35340683e+00,  1.76714587e+00,  4.26931026e-01,\n",
       "         3.33616023e-01,  1.14157248e-01,  2.71310634e-02, -3.52417632e-02,\n",
       "         1.71530529e-01,  9.14603849e-02, -4.03460247e-01,  7.26528396e-01,\n",
       "         1.66108048e+00,  3.23880294e-01,  3.41603389e-01,  1.64301770e-01,\n",
       "         7.25926593e-02, -1.50024676e-01,  1.25704543e-01,  3.25618579e-01,\n",
       "        -2.90628248e-01,  5.49953199e-01,  1.22954019e+00,  4.30735974e-01,\n",
       "         3.28326389e-01,  1.27352981e-01, -3.18461440e-02, -4.03375362e-02,\n",
       "        -1.79755361e-01,  1.04845442e-01,  4.26686024e-01,  7.18095849e-01,\n",
       "         1.66108048e+00,  3.27437964e-01,  3.45353346e-01,  1.74446741e-01,\n",
       "        -7.79413008e-02, -1.54053258e-01, -1.29622493e-01,  3.35880647e-01,\n",
       "         3.00817664e-01,  5.48053847e-01,  1.22954019e+00, -7.42580236e-03,\n",
       "         3.69033693e-03, -2.99841480e-03, -8.27460752e-03, -2.69764936e-03,\n",
       "         3.17646574e-03, -7.70124779e-03, -3.69496891e-03,  4.89286876e-03,\n",
       "        -6.32146131e-03, -2.53987064e-03,  3.39230253e-03, -3.32287531e-03,\n",
       "        -3.78360885e-03,  4.97770641e-03, -6.30975884e-03, -1.97271435e-03,\n",
       "         3.38092725e-03, -4.31929025e-03,  4.22770563e-03, -2.58230574e-03,\n",
       "        -5.48373025e-03, -2.16767176e-03,  3.06546052e-03, -4.32646635e-03,\n",
       "         3.72848738e-03, -2.57465136e-03, -5.67989593e-03, -2.16473010e-03,\n",
       "         3.07340718e-03, -4.32646635e-03,  3.72848738e-03, -2.57465136e-03,\n",
       "        -5.67989593e-03, -2.16473010e-03,  3.07340718e-03, -3.04010684e-03,\n",
       "        -1.25240772e-02, -2.60493881e-03, -7.18206316e-03, -2.14910718e-03,\n",
       "         3.55172461e-03, -2.99080091e-03, -8.07270077e-03, -2.67957568e-03,\n",
       "        -5.42540485e-03, -2.16960390e-03,  3.48975886e-03, -2.99080091e-03,\n",
       "        -8.07270077e-03, -2.67957568e-03, -5.42540485e-03, -2.16960390e-03,\n",
       "         3.48975886e-03, -5.49274611e-03, -4.16352355e-05,  2.81870390e-03,\n",
       "        -7.28359923e-03, -1.56021289e-03,  3.57687127e-03, -5.52628634e-03,\n",
       "        -2.03058364e-03,  4.83889851e-03, -7.30794950e-03, -1.88208129e-03,\n",
       "         3.25957691e-03, -1.42352757e-02,  2.53315578e-03, -1.11447666e-02,\n",
       "        -9.13975766e-03, -6.34035575e-03,  4.41708097e-03, -1.42327733e-02,\n",
       "         1.98352051e-03, -1.16836768e-02, -9.14753444e-03, -6.24977869e-03,\n",
       "         4.32466520e-03,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2035ecfe-44e4-4ab0-b4bb-7213f9d50c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_position': np.float64(-0.001028115287979985),\n",
       " 'y_position': np.float64(0.0018464669009018685),\n",
       " 'tendon_length': array([0.00430053, 0.00486432]),\n",
       " 'tendon_velocity': array([ 0.00415399, -0.00760405]),\n",
       " 'distance_from_origin': np.float64(0.0021134003552342653)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de32bf7-40c6-40e3-bbd3-b1131e598b18",
   "metadata": {},
   "source": [
    "As we can see, the humanoid environment gives us a TON of observations! We get dozens of variables representing various positions and velocities of body parts, the center of mass, and a lot of other variables I hardly understand. For an exhaustive list, see the [doc](https://gymnasium.farama.org/environments/mujoco/humanoid/#observation-space). The fact that there are so many variables here is what makes learning latent observations so obvious!\n",
    "\n",
    "We also get some nice summary stats in info, but we aren't going to include them in our scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00bc4518-6772-473b-8b89-5984a755ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rollout_data(env_name: str, out_dir: str, n_timesteps: int=10000, print_n_episodes: int=1000):\n",
    "    \"\"\"Simulates `n_timesteps` in the `env_name` environment, saving observations to an .npy file at `out_dir`.\"\"\"\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    obs, info = env.reset()\n",
    "    observations = []\n",
    "    episode_count = 0\n",
    "\n",
    "    for timestep in range(n_timesteps):  # Run for n_timesteps or until the episode ends\n",
    "        action = env.action_space.sample() #select random action\n",
    "        obs, reward, terminated, truncated, info = env.step(action) #execute and get results\n",
    "        observations.append(obs) #save observation\n",
    "        if terminated or truncated: #check for game over, if so reset env\n",
    "            episode_count+=1\n",
    "            if episode_count % print_n_episodes == 0: print(f\"finished {episode_count} episodes\") #provide update on training\n",
    "            observation, info = env.reset()\n",
    "        env.close()\n",
    "    np_obs = np.array(observations)\n",
    "    print(f\"final observations has shape: {np_obs.shape}\")\n",
    "    np.save(f'{out_dir}/{env_name}_{n_timesteps}_rollout_observations.npy', np_obs) #load with: new_obs = np.load(\"../data/processed/Humanoid-v5_10000_rollout_observations.npy\")\n",
    "    return np_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c936e8b4-1012-48af-9f9d-a1c0fb0da2ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 100 episodes\n",
      "finished 200 episodes\n",
      "finished 300 episodes\n",
      "finished 400 episodes\n",
      "final observations has shape: (10000, 348)\n"
     ]
    }
   ],
   "source": [
    "humanoid_obs = collect_rollout_data('Humanoid-v5', \"../data/processed\", 10000, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60900362-cf6f-4514-9708-4fad349d3e9f",
   "metadata": {},
   "source": [
    "## Training the VAE\n",
    "\n",
    "Now that we have an easy function for gathering observations, we need to train the VAE module of our world model which will compress the observation space into latent space with fewer dimensions.\n",
    "\n",
    "Note that the original World Model implementation worked with tensorflow 1.18.0. This is incredibly outdated (worked with Python 3.5), so let's get a newer version (tensorflow 2.19.0). Additionally, we need to change the structure of the VAE from working with images to working with a vector of observation data! Luckily, ChatGPT is very good at updating code (or it will be very obvious if it is not!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1389404-530a-43a4-862c-0403f544239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, saving\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "class MLPVAE(Model):\n",
    "    def __init__(self, input_dim=348, z_size=32, kl_tolerance=0.5):\n",
    "        super(MLPVAE, self).__init__()\n",
    "        self.z_size = z_size\n",
    "        self.kl_tolerance = kl_tolerance\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(input_dim,)),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(2 * z_size),  # output both mu and logvar\n",
    "        ])\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(z_size,)),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(input_dim, activation='linear'),  # output same shape as input\n",
    "        ])\n",
    "\n",
    "    def sample_z(self, mu, logvar):\n",
    "        eps = tf.random.normal(shape=tf.shape(mu))\n",
    "        sigma = tf.exp(0.5 * logvar)\n",
    "        return mu + sigma * eps\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = tf.split(h, num_or_size_splits=2, axis=1)\n",
    "        logvar = tf.clip_by_value(logvar, -10.0, 10.0)  # helps with exploding values\n",
    "        return mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def call(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.sample_z(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "    def compute_loss(self, x):\n",
    "        x_recon, mu, logvar = self(x)\n",
    "        recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_recon), axis=1))\n",
    "        kl_loss = -0.5 * tf.reduce_sum(1 + logvar - tf.square(mu) - tf.exp(logvar), axis=1)\n",
    "        kl_loss = tf.maximum(kl_loss, self.kl_tolerance * self.z_size)\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        total_loss = recon_loss + kl_loss\n",
    "        return total_loss, recon_loss, kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d97e9ff-74f4-4a65-b397-11e381a66fb6",
   "metadata": {},
   "source": [
    "Let's also write a training function for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ac2fe07-2e6d-4246-a75a-348182aac96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(x_train, batch_size=64, shuffle_buffer=10000):\n",
    "    # Assuming x_train is a NumPy array of shape [n_samples, 348]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(x_train.astype(np.float32))\n",
    "    dataset = dataset.shuffle(shuffle_buffer).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def train_vae(model, dataset, epochs=10, learning_rate=1e-4):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        total_batches = 0\n",
    "        for x_batch in dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, recon_loss, kl_loss = model.compute_loss(x_batch)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            total_loss += loss.numpy()\n",
    "            total_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / total_batches\n",
    "        print(f\"Epoch {epoch+1}: avg loss = {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0227816c-8f87-4374-860d-49162c0256d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg loss = 289430.8125\n",
      "Epoch 2: avg loss = 119881.2266\n",
      "Epoch 3: avg loss = 68145.3984\n",
      "Epoch 4: avg loss = 56953.0703\n",
      "Epoch 5: avg loss = 51048.7578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 21:55:19.295242: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: avg loss = 44877.3750\n",
      "Epoch 7: avg loss = 38613.2305\n",
      "Epoch 8: avg loss = 33449.9492\n",
      "Epoch 9: avg loss = 29071.9258\n",
      "Epoch 10: avg loss = 24597.8613\n",
      "Epoch 11: avg loss = 21002.9824\n",
      "Epoch 12: avg loss = 18676.8027\n",
      "Epoch 13: avg loss = 17292.6426\n",
      "Epoch 14: avg loss = 15747.7051\n",
      "Epoch 15: avg loss = 14375.2168\n",
      "Epoch 16: avg loss = 13334.0107\n",
      "Epoch 17: avg loss = 12652.3604\n",
      "Epoch 18: avg loss = 11696.5762\n",
      "Epoch 19: avg loss = 11132.5664\n",
      "Epoch 20: avg loss = 10717.4766\n"
     ]
    }
   ],
   "source": [
    "# x_train should be a NumPy array of shape (n_samples, 348)\n",
    "x_train = humanoid_obs\n",
    "x_train = (x_train - np.mean(x_train, axis=0)) / (np.std(x_train, axis=0) + 1e-6)\n",
    "\n",
    "dataset = create_dataset(humanoid_obs, batch_size=64)\n",
    "vae = MLPVAE(input_dim=348, z_size=32)\n",
    "train_vae(vae, dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4012c-cd27-45fd-bbc7-82e4a1be44cd",
   "metadata": {},
   "source": [
    "Loss is going down! At first I got a tons of NAN values, but it's because I wasn't normalizing the input data and I also needed to clip the logvar values we were getting as a result of the encoding process. If you get NANs again, a lower learning rate could help too. Onto saving the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1ae0e21-af4e-4a66-b0a0-8be8d82d5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.save_weights('../models/vae/humanoid_10000_vae_model.weights.h5') #save ONLY weights -- much simpler than serializing the entire object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8abe6c45-c5ae-4854-989f-fa2695cbf459",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_vae = MLPVAE(input_dim=348, z_size=32) #instantiate new model object \n",
    "new_vae(tf.zeros((1, 348))) #invoke it to build its shape \n",
    "new_vae.load_weights('../models/vae/humanoid_10000_vae_model.weights.h5') #now load weights into empty vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d3e5358-9331-49b2-87c9-bba5693775d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MLPVAE name=mlpvae_4, built=True>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "474e51af-98f1-4306-8971-c5d3c8edceb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MLPVAE name=mlpvae_3, built=True>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d1e06-9bb3-4ffd-bcdd-1e77bc6db66c",
   "metadata": {},
   "source": [
    "## Training the MDN-RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs757-final-project",
   "language": "python",
   "name": "cs757-final-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
