{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98bd1649-aa3f-4c0c-a7a2-e9a7d07e76c8",
   "metadata": {},
   "source": [
    "# World Model Core\n",
    "*Sean Steinle, Kiya Aminfar*\n",
    "\n",
    "This notebook walks through the core aspects of world models, developing crucial pieces of code sequentially. Not that this code isn't meant for scale -- instead, this is for a demonstration of how we developed the code that we did.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Collecting Rollout Data](#Collecting-Rollout-Data)\n",
    "2. [Training the VAE](#Training-the-VAE)\n",
    "3. [Training the MDN-RNN](#Training-the-MDN-RNN)\n",
    "    - [Prepping Rollout Data for the MDN-RNN](#Prepping-Rollout-Data-for-the-MDN-RNN)\n",
    "    - [Core Training](#Core-Training)\n",
    "4. [Training the Controller](#Training-the-Controller)\n",
    "5. [Early Results](#Early-Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21544f3f-d6e2-4fda-8b1d-4e6b51e934bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7847949-423a-47be-a72d-62322c9143ee",
   "metadata": {},
   "source": [
    "## Collecting Rollout Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fe9254-55f2-4148-84fc-aa3d9452e7af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's begin by creating an instance of our humanoid environment and checking out what basic observations look like.\n",
    "env = gym.make('Humanoid-v5', render_mode=\"rgb_array\")\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47c4e48-4af5-4ca9-be13-59301b800e17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((348,),\n",
       " array([ 1.39300131e+00,  9.95082371e-01,  3.87763039e-03, -2.46595386e-03,\n",
       "         1.68996662e-03, -9.42309819e-03, -6.06253604e-03, -4.55935851e-03,\n",
       "         1.81377671e-03,  9.27322084e-03,  1.20513263e-03,  9.15671483e-03,\n",
       "        -5.34892928e-03, -2.89313763e-03, -4.67409161e-03,  4.80980385e-03,\n",
       "         3.31908782e-04,  3.80452899e-03,  7.95388306e-03, -5.40410374e-03,\n",
       "        -5.11927945e-03,  6.37732803e-03, -1.33475781e-03,  4.30260742e-03,\n",
       "         1.09733892e-03, -1.90648726e-03, -1.09611434e-03, -9.08495783e-03,\n",
       "         7.41654441e-03,  9.75929267e-03,  9.96725691e-03,  7.37010286e-03,\n",
       "         4.84701979e-03, -1.81094195e-03, -1.26663294e-03,  5.95643963e-03,\n",
       "        -7.36869602e-03,  8.03223366e-03,  1.65738149e-03,  6.56168602e-03,\n",
       "         1.49662101e-03, -1.40926562e-03, -9.51879738e-03,  4.21087831e-03,\n",
       "        -6.92943150e-03,  2.30151498e+00,  2.28678620e+00,  4.58295022e-02,\n",
       "        -5.77727533e-04,  9.55042464e-02,  1.52667775e-02, -2.01205557e-01,\n",
       "        -2.94559727e-02,  4.35398627e+00,  8.90746237e+00,  9.50909542e-02,\n",
       "         9.07806755e-02,  1.16536718e-02, -1.02736259e-04,  1.23812947e-02,\n",
       "         4.06656499e-04, -6.37832410e-02, -2.34243652e-03,  4.39088410e-01,\n",
       "         2.26194671e+00,  5.77900930e-02,  4.36273868e-02,  6.61485077e-02,\n",
       "        -1.96294143e-04,  8.69985815e-03, -7.65109240e-05, -3.02501530e-01,\n",
       "        -5.62288042e-04,  1.90292218e-01,  6.61619413e+00,  2.73014927e-01,\n",
       "         2.33187391e-01,  5.29372325e-02, -1.01455663e-02, -1.82455658e-02,\n",
       "        -7.88059147e-02, -1.07240585e-01, -4.47744317e-01, -8.60680823e-01,\n",
       "         4.75175093e+00,  9.30180593e-01,  9.09753505e-01,  2.47737633e-02,\n",
       "        -3.42460368e-03, -2.13053903e-02, -1.35544755e-01, -3.91797182e-02,\n",
       "        -2.40588175e-01, -1.55422439e+00,  2.75569617e+00,  1.04842168e+00,\n",
       "         1.03539857e+00,  1.72747794e-02, -1.34665327e-03, -1.19207105e-02,\n",
       "        -1.16501689e-01, -1.56044711e-02, -1.52503263e-01, -1.34997426e+00,\n",
       "         1.76714587e+00,  2.73872707e-01,  2.31720048e-01,  5.43885663e-02,\n",
       "         9.34978669e-03, -1.58793916e-02,  8.12657995e-02, -9.66816548e-02,\n",
       "         4.57913128e-01, -8.57738400e-01,  4.75175093e+00,  9.32005377e-01,\n",
       "         9.07652174e-01,  2.81955320e-02,  2.72434217e-03, -1.54248174e-02,\n",
       "         1.47173100e-01, -2.88645205e-02,  2.60774071e-01, -1.55258769e+00,\n",
       "         2.75569617e+00,  1.05002116e+00,  1.03366218e+00,  2.04156969e-02,\n",
       "         8.13126411e-04, -6.44301756e-03,  1.29944109e-01, -8.44076079e-03,\n",
       "         1.70235008e-01, -1.34890114e+00,  1.76714587e+00,  4.27808453e-01,\n",
       "         3.28590655e-01,  1.23084813e-01,  3.06484933e-02, -3.92717963e-02,\n",
       "         1.76605889e-01,  1.02159298e-01, -4.19164188e-01,  7.18904176e-01,\n",
       "         1.66108048e+00,  3.24023696e-01,  3.41881011e-01,  1.69948385e-01,\n",
       "         7.56421821e-02, -1.51809394e-01,  1.27259456e-01,  3.31653397e-01,\n",
       "        -2.96200048e-01,  5.46756361e-01,  1.22954019e+00,  4.28073532e-01,\n",
       "         3.33129795e-01,  1.18086313e-01, -2.94731572e-02, -3.85729634e-02,\n",
       "        -1.73850299e-01,  9.95761693e-02,  4.09575682e-01,  7.24658086e-01,\n",
       "         1.66108048e+00,  3.24305988e-01,  3.44262937e-01,  1.66314140e-01,\n",
       "        -7.35627857e-02, -1.52081997e-01, -1.25425085e-01,  3.30113173e-01,\n",
       "         2.90062799e-01,  5.50421188e-01,  1.22954019e+00, -1.85780240e-03,\n",
       "        -1.03163424e-03, -9.10257766e-03, -8.38675892e-04,  3.27794098e-03,\n",
       "         1.11222029e-03, -1.86550483e-03,  8.66921796e-03, -1.60975898e-03,\n",
       "        -3.36381543e-03,  3.49453840e-03,  8.29198722e-04,  8.09977220e-03,\n",
       "         8.60786857e-03, -1.42081352e-03, -3.35599354e-03,  4.78649612e-03,\n",
       "         8.36150463e-04,  1.53821957e-02,  6.72731940e-03,  3.55602310e-03,\n",
       "        -3.87509500e-03,  4.83234290e-03,  1.61305637e-03,  1.53781266e-02,\n",
       "         7.99393031e-03,  3.56229055e-03, -3.37651658e-03,  4.83406422e-03,\n",
       "         1.58888914e-03,  1.53781266e-02,  7.99393031e-03,  3.56229055e-03,\n",
       "        -3.37651658e-03,  4.83406422e-03,  1.58888914e-03,  2.02933891e-03,\n",
       "         1.66134762e-02,  5.90247490e-03, -2.54004496e-03,  5.02873497e-03,\n",
       "         1.24769949e-03,  2.02420232e-03,  1.49561640e-02,  5.88821533e-03,\n",
       "        -3.19350203e-03,  5.03053772e-03,  1.27356450e-03,  2.02420232e-03,\n",
       "         1.49561640e-02,  5.88821533e-03, -3.19350203e-03,  5.03053772e-03,\n",
       "         1.27356450e-03,  3.47597673e-03,  5.77487891e-04, -5.32671696e-03,\n",
       "        -2.31895161e-03,  6.08723293e-03,  2.00604826e-03,  3.47722614e-03,\n",
       "         1.58199465e-03, -6.31514713e-03, -2.30634885e-03,  6.24863518e-03,\n",
       "         2.17009161e-03, -9.63552376e-03,  5.82577223e-03, -1.00092346e-02,\n",
       "        -4.52445043e-03, -7.50093654e-04,  2.26486866e-03, -9.65482721e-03,\n",
       "         1.07090055e-02, -5.09286590e-03, -4.46691933e-03, -1.54727201e-03,\n",
       "         3.05690007e-03,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.shape, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2035ecfe-44e4-4ab0-b4bb-7213f9d50c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_position': np.float64(-0.0016523157076843691),\n",
       " 'y_position': np.float64(-0.007591421744894626),\n",
       " 'tendon_length': array([0.0094839 , 0.00795158]),\n",
       " 'tendon_velocity': array([-0.00637485,  0.00054431]),\n",
       " 'distance_from_origin': np.float64(0.007769158983231034)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de32bf7-40c6-40e3-bbd3-b1131e598b18",
   "metadata": {},
   "source": [
    "As we can see, the humanoid environment gives us a TON of observations! We get dozens of variables representing various positions and velocities of body parts, the center of mass, and a lot of other variables I hardly understand. For an exhaustive list, see the [doc](https://gymnasium.farama.org/environments/mujoco/humanoid/#observation-space). The fact that there are so many variables here is what makes learning latent observations so obvious!\n",
    "\n",
    "We also get some nice summary stats in info, but we aren't going to include them in our scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00bc4518-6772-473b-8b89-5984a755ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rollout_data(env_name: str, out_dir: str, n_timesteps: int=10000, print_n_episodes: int=1000):\n",
    "    \"\"\"Simulates `n_timesteps` in the `env_name` environment, saving observations, rewards, and actions to a triplet of .npy files at `out_dir`.\"\"\"\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    obs, info = env.reset()\n",
    "    observations, rewards, actions, episode_boundaries = [], [] , [], []\n",
    "    episode_count = 0\n",
    "\n",
    "    for timestep in range(n_timesteps):  # Run for n_timesteps or until the episode ends\n",
    "        action = env.action_space.sample() #select random action\n",
    "        obs, reward, terminated, truncated, info = env.step(action) #execute and get results\n",
    "        observations.append(obs) #save observation\n",
    "        rewards.append(reward) #save reward\n",
    "        actions.append(action) #save action\n",
    "        if terminated or truncated: #check for game over, if so reset env\n",
    "            episode_boundaries.append(timestep) #save timestep of each episode's boundary\n",
    "            episode_count+=1\n",
    "            if episode_count % print_n_episodes == 0: print(f\"finished {episode_count} episodes\") #provide update on training\n",
    "            observation, info = env.reset()\n",
    "        env.close()\n",
    "    np_obs, np_rewards, np_actions, np_episode_boundaries = np.array(observations), np.array(rewards), np.array(actions), np.array(episode_boundaries)\n",
    "    print(f\"observations has shape: {np_obs.shape}\\trewards has shape: {np_rewards.shape}\\tactions has shape: {np_actions.shape}\\tboundaries has shape: {episode_boundaries}\")\n",
    "    np.save(f'{out_dir}/{env_name}_{n_timesteps}_rollout_observations.npy', np_obs) #load with: new_obs = np.load(\"../data/processed/Humanoid-v5_10000_rollout_observations.npy\")\n",
    "    np.save(f'{out_dir}/{env_name}_{n_timesteps}_rollout_rewards.npy', np_rewards)\n",
    "    np.save(f'{out_dir}/{env_name}_{n_timesteps}_rollout_actions.npy', np_actions)\n",
    "    np.save(f'{out_dir}/{env_name}_{n_timesteps}_rollout_boundaries.npy', np_episode_boundaries)\n",
    "    return np_obs, np_rewards, np_actions, np_episode_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c936e8b4-1012-48af-9f9d-a1c0fb0da2ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 100 episodes\n",
      "finished 200 episodes\n",
      "finished 300 episodes\n",
      "finished 400 episodes\n",
      "observations has shape: (10000, 348)\trewards has shape: (10000,)\tactions has shape: (10000, 17)\n"
     ]
    }
   ],
   "source": [
    "humanoid_obs, humanoid_rewards, humanoid_actions, humanoid_boundaries = collect_rollout_data('Humanoid-v5', \"../data/processed\", 10000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bea542a-9109-4bd2-b80f-8e4775dde9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.9154845 , 4.91313937, 4.94133224, ..., 2.56266492, 3.89328895,\n",
       "       5.00908928])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humanoid_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26d613d5-e3a6-43f6-abf0-c3cfe81bd712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08674568,  0.18042727,  0.24072039, ...,  0.14242245,\n",
       "        -0.16913189, -0.29231656],\n",
       "       [-0.29700246, -0.08621656, -0.36000764, ...,  0.3989401 ,\n",
       "         0.25781026, -0.16689824],\n",
       "       [ 0.11839256,  0.27461025,  0.07927635, ...,  0.08514802,\n",
       "         0.13366902, -0.23283356],\n",
       "       ...,\n",
       "       [-0.3481346 , -0.1078826 , -0.2464141 , ...,  0.327872  ,\n",
       "         0.1691019 , -0.13490571],\n",
       "       [ 0.22734375,  0.12003687, -0.34475392, ..., -0.2912965 ,\n",
       "        -0.1930455 , -0.18568286],\n",
       "       [ 0.38920617,  0.13925335, -0.09383011, ...,  0.02945893,\n",
       "        -0.26485687,  0.13945284]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humanoid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0536b8fe-b7c4-4b15-9ec7-968f2c8b6845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  24,   44,   63,   93,  117,  138,  182,  209,  226,  268,  298,\n",
       "        315,  351,  371,  403,  422,  443,  463,  488,  517,  535,  557,\n",
       "        576,  595,  614,  642,  662,  689,  707,  727,  745,  766,  787,\n",
       "        807,  845,  864,  890,  916,  940,  958,  986, 1007, 1033, 1062,\n",
       "       1089, 1110, 1131, 1153, 1174, 1193, 1213, 1238, 1263, 1288, 1324,\n",
       "       1362, 1381, 1410, 1429, 1446, 1463, 1482, 1524, 1553, 1572, 1589,\n",
       "       1622, 1642, 1662, 1679, 1704, 1736, 1756, 1786, 1805, 1826, 1854,\n",
       "       1873, 1915, 1952, 1975, 1997, 2039, 2059, 2086, 2121, 2148, 2183,\n",
       "       2210, 2239, 2258, 2309, 2355, 2374, 2400, 2430, 2449, 2483, 2521,\n",
       "       2544, 2570, 2606, 2632, 2663, 2693, 2712, 2740, 2770, 2791, 2822,\n",
       "       2845, 2866, 2889, 2909, 2928, 2959, 2977, 2996, 3016, 3037, 3059,\n",
       "       3099, 3117, 3159, 3178, 3198, 3228, 3245, 3264, 3282, 3302, 3321,\n",
       "       3338, 3366, 3387, 3428, 3446, 3473, 3503, 3535, 3556, 3578, 3596,\n",
       "       3620, 3639, 3665, 3684, 3719, 3742, 3760, 3790, 3811, 3840, 3877,\n",
       "       3909, 3928, 3968, 3985, 4014, 4044, 4070, 4089, 4110, 4131, 4149,\n",
       "       4167, 4189, 4206, 4226, 4257, 4279, 4316, 4333, 4374, 4394, 4417,\n",
       "       4435, 4460, 4480, 4513, 4534, 4551, 4582, 4627, 4646, 4664, 4697,\n",
       "       4716, 4734, 4753, 4779, 4801, 4821, 4838, 4858, 4875, 4892, 4915,\n",
       "       4935, 4959, 4993, 5025, 5042, 5072, 5096, 5113, 5143, 5162, 5180,\n",
       "       5199, 5217, 5235, 5263, 5309, 5345, 5363, 5380, 5401, 5429, 5450,\n",
       "       5477, 5505, 5529, 5548, 5573, 5605, 5624, 5653, 5671, 5694, 5726,\n",
       "       5750, 5776, 5793, 5820, 5837, 5882, 5908, 5924, 5953, 5977, 6002,\n",
       "       6021, 6051, 6070, 6089, 6111, 6140, 6160, 6191, 6215, 6254, 6287,\n",
       "       6312, 6346, 6372, 6388, 6437, 6454, 6477, 6528, 6554, 6576, 6592,\n",
       "       6610, 6639, 6666, 6686, 6706, 6726, 6748, 6781, 6806, 6852, 6869,\n",
       "       6888, 6908, 6927, 6949, 6967, 6998, 7017, 7035, 7054, 7113, 7133,\n",
       "       7165, 7193, 7222, 7252, 7280, 7297, 7318, 7345, 7362, 7387, 7404,\n",
       "       7424, 7443, 7470, 7492, 7520, 7540, 7571, 7596, 7624, 7651, 7674,\n",
       "       7697, 7722, 7741, 7772, 7798, 7822, 7842, 7861, 7883, 7900, 7918,\n",
       "       7945, 7963, 7981, 8006, 8032, 8051, 8077, 8108, 8144, 8177, 8215,\n",
       "       8233, 8260, 8281, 8298, 8330, 8366, 8392, 8411, 8433, 8459, 8483,\n",
       "       8530, 8547, 8568, 8589, 8618, 8638, 8658, 8704, 8727, 8753, 8774,\n",
       "       8797, 8815, 8833, 8853, 8885, 8903, 8930, 8954, 8972, 8997, 9014,\n",
       "       9051, 9072, 9092, 9112, 9140, 9172, 9195, 9212, 9229, 9247, 9270,\n",
       "       9289, 9307, 9340, 9375, 9393, 9413, 9434, 9458, 9485, 9515, 9544,\n",
       "       9562, 9581, 9601, 9626, 9643, 9663, 9684, 9702, 9722, 9741, 9761,\n",
       "       9782, 9806, 9831, 9850, 9870, 9897, 9940, 9959, 9986])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humanoid_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60900362-cf6f-4514-9708-4fad349d3e9f",
   "metadata": {},
   "source": [
    "## Training the VAE\n",
    "\n",
    "Now that we have an easy function for gathering experiences in the environment, we need to train the VAE module of our world model which will compress the observation space into latent space with fewer dimensions.\n",
    "\n",
    "Note that the original World Model implementation worked with tensorflow 1.18.0. This is incredibly outdated (worked with Python 3.5), so let's get a newer version (tensorflow 2.19.0). Additionally, we need to change the structure of the VAE from working with images to working with a vector of observation data! Luckily, ChatGPT is very good at updating code (or it will be very obvious if it is not!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1389404-530a-43a4-862c-0403f544239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 10:27:00.743408: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-09 10:27:00.770285: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746800820.791959   31576 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746800820.798624   31576 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746800820.816920   31576 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746800820.816946   31576 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746800820.816949   31576 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746800820.816951   31576 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-09 10:27:00.822721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, saving\n",
    "\n",
    "@saving.register_keras_serializable()\n",
    "class MLPVAE(Model):\n",
    "    def __init__(self, input_dim=348, z_size=32, kl_tolerance=0.5):\n",
    "        super(MLPVAE, self).__init__()\n",
    "        self.z_size = z_size\n",
    "        self.kl_tolerance = kl_tolerance\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(input_dim,)),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(2 * z_size),  # output both mu and logvar\n",
    "        ])\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(z_size,)),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dense(input_dim, activation='linear'),  # output same shape as input\n",
    "        ])\n",
    "\n",
    "    def sample_z(self, mu, logvar):\n",
    "        eps = tf.random.normal(shape=tf.shape(mu))\n",
    "        sigma = tf.exp(0.5 * logvar)\n",
    "        return mu + sigma * eps\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = tf.split(h, num_or_size_splits=2, axis=1)\n",
    "        logvar = tf.clip_by_value(logvar, -10.0, 10.0)  # helps with exploding values\n",
    "        return mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def call(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.sample_z(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "    def compute_loss(self, x):\n",
    "        x_recon, mu, logvar = self(x)\n",
    "        recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_recon), axis=1))\n",
    "        kl_loss = -0.5 * tf.reduce_sum(1 + logvar - tf.square(mu) - tf.exp(logvar), axis=1)\n",
    "        kl_loss = tf.maximum(kl_loss, self.kl_tolerance * self.z_size)\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        total_loss = recon_loss + kl_loss\n",
    "        return total_loss, recon_loss, kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d97e9ff-74f4-4a65-b397-11e381a66fb6",
   "metadata": {},
   "source": [
    "Let's also write a training function for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ac2fe07-2e6d-4246-a75a-348182aac96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(x_train, batch_size=64, shuffle_buffer=10000):\n",
    "    # Assuming x_train is a NumPy array of shape [n_samples, 348]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(x_train.astype(np.float32))\n",
    "    dataset = dataset.shuffle(shuffle_buffer).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def train_vae(model, dataset, epochs=10, learning_rate=1e-4):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        total_batches = 0\n",
    "        for x_batch in dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, recon_loss, kl_loss = model.compute_loss(x_batch)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            total_loss += loss.numpy()\n",
    "            total_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / total_batches\n",
    "        print(f\"Epoch {epoch+1}: avg loss = {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0227816c-8f87-4374-860d-49162c0256d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 10:27:07.306695: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/seansteinle/.pyenv/versions/cs757-final-project/lib/python3.10/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n",
      "2025-05-09 10:27:24.075387: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg loss = 289024.7188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 10:27:40.264828: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: avg loss = 123538.1016\n",
      "Epoch 3: avg loss = 67821.8672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 10:28:09.838940: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: avg loss = 58037.1836\n",
      "Epoch 5: avg loss = 52092.7578\n",
      "Epoch 6: avg loss = 46350.1094\n",
      "Epoch 7: avg loss = 39106.0586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 10:29:02.727875: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: avg loss = 33068.6367\n",
      "Epoch 9: avg loss = 28321.2461\n",
      "Epoch 10: avg loss = 24051.6816\n",
      "Epoch 11: avg loss = 20940.8828\n",
      "Epoch 12: avg loss = 18792.0293\n",
      "Epoch 13: avg loss = 16970.7285\n",
      "Epoch 14: avg loss = 15694.1816\n",
      "Epoch 15: avg loss = 14791.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 10:30:57.359811: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: avg loss = 13650.3232\n",
      "Epoch 17: avg loss = 12947.7314\n",
      "Epoch 18: avg loss = 12331.4980\n",
      "Epoch 19: avg loss = 11848.9102\n",
      "Epoch 20: avg loss = 11423.7061\n"
     ]
    }
   ],
   "source": [
    "# x_train should be a NumPy array of shape (n_samples, 348)\n",
    "x_train = humanoid_obs\n",
    "x_train = (x_train - np.mean(x_train, axis=0)) / (np.std(x_train, axis=0) + 1e-6)\n",
    "\n",
    "dataset = create_dataset(humanoid_obs, batch_size=64)\n",
    "vae = MLPVAE(input_dim=348, z_size=32)\n",
    "train_vae(vae, dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4012c-cd27-45fd-bbc7-82e4a1be44cd",
   "metadata": {},
   "source": [
    "Loss is going down! At first I got a tons of NAN values, but it's because I wasn't normalizing the input data and I also needed to clip the logvar values we were getting as a result of the encoding process. If you get NANs again, a lower learning rate could help too. Onto saving the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1ae0e21-af4e-4a66-b0a0-8be8d82d5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.save_weights('../models/vae/humanoid_10000_vae_model.weights.h5') #save ONLY weights -- much simpler than serializing the entire object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8abe6c45-c5ae-4854-989f-fa2695cbf459",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_vae = MLPVAE(input_dim=348, z_size=32) #instantiate new model object \n",
    "new_vae(tf.zeros((1, 348))) #invoke it to build its shape \n",
    "new_vae.load_weights('../models/vae/humanoid_10000_vae_model.weights.h5') #now load weights into empty vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d3e5358-9331-49b2-87c9-bba5693775d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MLPVAE name=mlpvae_1, built=True>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "474e51af-98f1-4306-8971-c5d3c8edceb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MLPVAE name=mlpvae, built=True>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d1e06-9bb3-4ffd-bcdd-1e77bc6db66c",
   "metadata": {},
   "source": [
    "## Training the MDN-RNN\n",
    "\n",
    "Now that we have a model which captures observations, we're theoretically ~1/3 done with the project! I say theoretically because this was probably the easiest part of the project. Now onto the meat of world models: capturing the transitions of our environment and training the MDN-RNN!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc9e9b5-64c8-4c11-8d0a-5ca8cdefa295",
   "metadata": {},
   "source": [
    "### Prepping Rollout Data for the MDN-RNN\n",
    "\n",
    "To train the MDN-RNN, we first need to enhance our basic rollout dataset with predictions of `mu` and `logvar` for each experience. Then we'll feed this information to the MDN-RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "960d5f8f-30a5-45e5-b013-48de1be22c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 348), (10000,), (10000, 17), (405,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can use the dataset records still in memory. Note that we only need the observations for now!\n",
    "humanoid_obs.shape, humanoid_rewards.shape, humanoid_actions.shape, humanoid_boundaries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51554a1a-a2db-4db8-8442-2a18e2b27427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MLPVAE name=mlpvae, built=True>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can also use the VAE still in memory!\n",
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e69fb5c-cda7-4adb-ab82-c2e902e7a80d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.40830562e+00,  9.99876874e-01, -9.51343451e-03,  1.18707539e-02,\n",
       "       -3.84923369e-03,  1.15938890e-02, -3.00163036e-02, -4.25435428e-03,\n",
       "        9.58761690e-04, -6.35710950e-02,  2.01602117e-02, -4.33043558e-02,\n",
       "        5.35531591e-03,  8.45089699e-02,  2.45519732e-02, -2.40603192e-02,\n",
       "       -1.01506478e-02,  1.46174317e-02, -8.65743713e-03,  1.21055954e-02,\n",
       "       -1.53897781e-02, -2.20527455e-02, -2.35265818e-01,  3.60738168e-02,\n",
       "       -1.82763673e-01, -3.03734843e-01,  1.64679568e+00, -4.48322248e-01,\n",
       "        9.44238863e-01, -4.28256359e+00,  2.59820444e-01, -6.46090300e-02,\n",
       "       -5.44796398e+00,  8.57359267e-01, -5.35796373e+00,  3.66024060e-01,\n",
       "        6.90749771e+00,  1.91271460e+00, -3.74138601e+00, -1.69903784e+00,\n",
       "        2.16734825e+00, -1.25681037e+00,  1.86683659e+00, -1.85226147e+00,\n",
       "       -2.09367945e+00,  2.30252275e+00,  2.28360708e+00,  4.34351997e-02,\n",
       "        7.03459228e-04,  3.90673226e-02, -4.73535097e-02, -9.34772193e-02,\n",
       "        9.25679374e-02,  4.35441233e+00,  8.90746237e+00,  9.54684769e-02,\n",
       "        9.05079200e-02,  1.11029316e-02,  2.79165758e-04,  1.01247092e-02,\n",
       "       -2.03173939e-03, -5.20600356e-02,  1.10394060e-02,  4.39927177e-01,\n",
       "        2.26194671e+00,  5.79906096e-02,  4.13027389e-02,  6.36159918e-02,\n",
       "        4.57401679e-04,  7.99749271e-03,  4.04101065e-04, -2.73458808e-01,\n",
       "        8.32534882e-03,  1.93558643e-01,  6.61619413e+00,  2.71589209e-01,\n",
       "        2.27949042e-01,  5.60882919e-02, -9.82042730e-03, -1.81173559e-02,\n",
       "       -8.25208722e-02, -1.00111730e-01, -4.66232320e-01, -8.46761827e-01,\n",
       "        4.75175093e+00,  9.27826624e-01,  9.02484887e-01,  3.37596677e-02,\n",
       "       -8.45669047e-03, -4.84077814e-02, -1.56226378e-01, -8.37133700e-02,\n",
       "       -2.77507335e-01, -1.54600637e+00,  2.75569617e+00,  1.04593699e+00,\n",
       "        1.02954305e+00,  2.60816455e-02, -7.41442950e-03, -5.40432785e-02,\n",
       "       -1.40310136e-01, -7.10397443e-02, -1.84437297e-01, -1.34435107e+00,\n",
       "        1.76714587e+00,  2.72577791e-01,  2.35689516e-01,  5.03017952e-02,\n",
       "        1.00240706e-02, -2.10618064e-02,  7.51499674e-02, -1.10775529e-01,\n",
       "        4.31995197e-01, -8.67467932e-01,  4.75175093e+00,  9.28805500e-01,\n",
       "        9.15624557e-01,  2.19814118e-02,  6.62713765e-03, -5.03925848e-02,\n",
       "        1.17839355e-01, -8.73822871e-02,  2.09772352e-01, -1.55747822e+00,\n",
       "        2.75569617e+00,  1.04713810e+00,  1.04080140e+00,  1.55654168e-02,\n",
       "        4.85177774e-03, -5.21165641e-02,  9.62784819e-02, -6.81255340e-02,\n",
       "        1.25852943e-01, -1.35188035e+00,  1.76714587e+00,  4.27798834e-01,\n",
       "        3.38900765e-01,  1.16252754e-01,  3.32379078e-02, -4.66806570e-02,\n",
       "        1.71030518e-01,  1.17159696e-01, -4.00946887e-01,  7.28940480e-01,\n",
       "        1.66108048e+00,  3.19660092e-01,  3.50724856e-01,  1.73621672e-01,\n",
       "        7.59529432e-02, -1.58511377e-01,  1.23116382e-01,  3.46210654e-01,\n",
       "       -2.85837542e-01,  5.47830804e-01,  1.22954019e+00,  4.28810725e-01,\n",
       "        3.26865553e-01,  1.31456666e-01, -3.67765922e-02, -4.80692531e-02,\n",
       "       -1.79922117e-01,  1.23521204e-01,  4.30180039e-01,  7.13441532e-01,\n",
       "        1.66108048e+00,  3.18385533e-01,  3.44421277e-01,  1.86709389e-01,\n",
       "       -8.32837946e-02, -1.58339996e-01, -1.28781233e-01,  3.53252703e-01,\n",
       "        3.05228158e-01,  5.35834799e-01,  1.22954019e+00, -3.01923187e-01,\n",
       "        1.64168981e+00, -4.72301352e-01, -9.85516827e-01, -1.05176726e-01,\n",
       "       -1.94242307e-01, -2.65555743e-01, -2.62540461e+00,  5.55868091e-01,\n",
       "        1.27967361e-01, -7.13959015e-02, -9.34307200e-02, -6.01499427e-03,\n",
       "       -2.62426893e+00,  5.59528296e-01,  1.27833665e-01, -3.77019397e-02,\n",
       "       -9.44049055e-02,  5.80674237e-02, -1.88565651e+00, -4.91972503e+00,\n",
       "        6.79300115e-01, -1.49787161e-01, -1.03064522e-01,  3.76206367e-01,\n",
       "        3.45902109e+00, -5.03349966e+00,  2.78153909e+00, -2.76767282e-01,\n",
       "       -1.89759799e-01,  3.76206367e-01,  3.45902109e+00, -5.03349966e+00,\n",
       "        2.78153909e+00, -2.76767282e-01, -1.89759799e-01, -1.19661631e-01,\n",
       "       -9.15724948e-01, -6.41834821e+00, -5.50905675e-01, -1.83978787e-01,\n",
       "       -1.19166591e-01,  1.81071407e-01,  2.81280264e+00, -6.52016751e+00,\n",
       "        9.14939006e-01, -3.05505843e-01, -2.39861328e-01,  1.81071407e-01,\n",
       "        2.81280264e+00, -6.52016751e+00,  9.14939006e-01, -3.05505843e-01,\n",
       "       -2.39861328e-01, -1.69854122e+00, -5.45072452e-01,  4.54529226e-01,\n",
       "       -1.63261635e-03, -8.19746036e-01, -3.97605004e-01, -1.70592031e+00,\n",
       "        3.19086333e-01, -4.58165288e-01,  1.81289020e-02, -6.67579767e-01,\n",
       "       -2.53690481e-01,  1.20660896e+00, -4.40654358e-01, -1.03008050e+00,\n",
       "       -2.25733009e-02,  6.62088440e-01, -4.54343247e-01,  1.23852365e+00,\n",
       "        1.05344278e+00,  4.36781336e-01,  1.08524392e-02,  4.15970413e-01,\n",
       "       -2.04382782e-01,  1.80427268e+01, -8.67456794e+00,  2.40720391e+01,\n",
       "        2.48305455e+01, -3.13292593e+01, -2.46740699e+01, -1.96625635e+01,\n",
       "       -1.23956241e-01,  3.80080789e+01,  8.71370226e+01, -4.98736441e+01,\n",
       "       -9.75592583e+00,  4.54537980e+00, -9.81456321e-01,  3.56056131e+00,\n",
       "       -4.22829725e+00, -7.30791390e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humanoid_obs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf611482-45e7-49f5-9fe6-378f2665986f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 32), (10000, 32))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#That's it! Just predict for our entire observation set.\n",
    "mu, logvar = vae.encode(humanoid_obs)\n",
    "mu, logvar = mu.numpy(), logvar.numpy() #cast to numpy (it's an EagerTensor)\n",
    "mu.shape, logvar.shape #makes sense -- a mean and stdev for each dimen of latent space for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cf82902-0901-43d1-b8bc-f467e3c51525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(humanoid_obs), type(mu.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3e0a85a-e3e4-477a-8f2a-bea624a053ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'../data/processed/Humanoid-v5_10000_rollout_mu.npy', mu)\n",
    "np.save(f'../data/processed/Humanoid-v5_10000_rollout_logvar.npy', logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "949bbfd5-6ead-409c-8c9f-645d631acf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 348),\n",
       " (10000,),\n",
       " (10000, 17),\n",
       " TensorShape([10000, 32]),\n",
       " TensorShape([10000, 32]),\n",
       " (405,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So we created the mu and logvar predictions, but we also need to chunk our data into episodes which we can train our model on.\n",
    "#This is where we can use our boundaries data!\n",
    "humanoid_obs.shape, humanoid_rewards.shape, humanoid_actions.shape, mu.shape, logvar.shape, humanoid_boundaries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81fd572f-7d58-45eb-8253-5d5f3d63b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of episode dict which contain each datatype of interest\n",
    "episodes = []\n",
    "start_index = 0\n",
    "for end_index in humanoid_boundaries:\n",
    "    edict = {}\n",
    "    edict['obs'] = humanoid_obs[start_index:end_index]\n",
    "    edict['rewards'] = humanoid_rewards[start_index:end_index]\n",
    "    edict['actions'] = humanoid_actions[start_index:end_index]\n",
    "    edict['mu'] = mu[start_index:end_index]\n",
    "    edict['logvar'] = logvar[start_index:end_index]\n",
    "    episodes.append(edict)\n",
    "    start_index = end_index #move window forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7e22037-7a32-4747-a911-851271dd30b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24, 348), (24,), (24, 17), (24, 32), (24, 32))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#great, first dimension matches for all data!\n",
    "episodes[0]['obs'].shape, episodes[0]['rewards'].shape, episodes[0]['actions'].shape, episodes[0]['mu'].shape, episodes[0]['logvar'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d4e24-7e60-4ffb-b4f4-bc8a1f79bf06",
   "metadata": {},
   "source": [
    "### Core Training\n",
    "\n",
    "Now that we have our mu and logvar arrays and also episode-wise aggregations of all of our data, we can train our MDN-RNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6dac2e64-1ab7-44a3-8e34-fcaca68210bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDNRNN(tf.keras.Model):\n",
    "    def __init__(self, latent_size, action_size, rnn_units=256, num_mixtures=5):\n",
    "        super(MDNRNN, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.action_size = action_size\n",
    "        self.rnn_units = rnn_units\n",
    "        self.num_mixtures = num_mixtures\n",
    "\n",
    "        # Inputs: [z_t, a_t] concatenated\n",
    "        self.input_layer = layers.Dense(rnn_units, activation='relu')\n",
    "\n",
    "        # RNN core\n",
    "        self.lstm = layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
    "\n",
    "        # Outputs: MDN parameters and optional done prediction\n",
    "        output_dim = num_mixtures * (2 * latent_size + 1)  # means, stds, logmix\n",
    "        self.mdn_output = layers.Dense(output_dim)\n",
    "        self.done_output = layers.Dense(1, activation='sigmoid')  # optional 'done' signal\n",
    "\n",
    "    def call(self, inputs, states=None, training=False):\n",
    "        \"\"\"\n",
    "        inputs: (z_t, a_t) of shape (batch, seq_len, latent_size + action_size)\n",
    "        states: initial LSTM states (optional)\n",
    "        returns: ((pi, mu, log_sigma), done_pred, final_states)\n",
    "        \"\"\"\n",
    "        x = self.input_layer(inputs)\n",
    "        x, final_h, final_c = self.lstm(x, initial_state=states, training=training)\n",
    "    \n",
    "        mdn_params = self.mdn_output(x)  # shape: (B, T, M * (2D + 1))\n",
    "        done_pred = self.done_output(x)\n",
    "    \n",
    "        # Reshape and split MDN params\n",
    "        B, T = tf.shape(mdn_params)[0], tf.shape(mdn_params)[1]\n",
    "        M = self.num_mixtures\n",
    "        D = self.latent_size\n",
    "    \n",
    "        # Reshape to (B, T, M, 2D + 1)\n",
    "        mdn_params = tf.reshape(mdn_params, [B, T, M, 2 * D + 1])\n",
    "    \n",
    "        # Split into pi, mu, log_sigma\n",
    "        pi = mdn_params[..., 0]                 # shape (B, T, M)\n",
    "        mu = mdn_params[..., 1:D+1]             # shape (B, T, M, D)\n",
    "        log_sigma = mdn_params[..., D+1:]       # shape (B, T, M, D)\n",
    "    \n",
    "        return (pi, mu, log_sigma), done_pred, [final_h, final_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "23da4a81-88c5-4b1d-b80e-0d6ecb3df67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "def compute_mdn_loss(targets, mdn_params):\n",
    "    \"\"\"\n",
    "    targets: shape (B, T, D)\n",
    "    mdn_params: tuple (pi, mu, log_sigma)\n",
    "        - pi: (B, T, M)\n",
    "        - mu, log_sigma: (B, T, M, D)\n",
    "    Returns scalar loss (average NLL over batch and time)\n",
    "    \"\"\"\n",
    "    pi, mu, log_sigma = mdn_params  # already split in model\n",
    "    B, T, D = tf.shape(targets)[0], tf.shape(targets)[1], tf.shape(targets)[2]\n",
    "    M = tf.shape(pi)[-1]\n",
    "\n",
    "    # Expand targets to shape (B, T, 1, D) to broadcast against (B, T, M, D)\n",
    "    targets_expanded = tf.expand_dims(targets, axis=2)\n",
    "\n",
    "    # Compute component-wise log-probabilities\n",
    "    dist = tfd.Normal(loc=mu, scale=tf.exp(log_sigma))  # shape: (B, T, M, D)\n",
    "    log_prob = dist.log_prob(targets_expanded)  # (B, T, M, D)\n",
    "    log_prob = tf.reduce_sum(log_prob, axis=-1)  # sum over D → shape: (B, T, M)\n",
    "\n",
    "    # Apply log(pi) with log-sum-exp trick\n",
    "    log_pi = tf.math.log_softmax(pi, axis=-1)  # shape: (B, T, M)\n",
    "    log_mix = log_pi + log_prob               # (B, T, M)\n",
    "\n",
    "    # LogSumExp over mixtures → log p(x)\n",
    "    log_likelihood = tf.reduce_logsumexp(log_mix, axis=-1)  # shape: (B, T)\n",
    "\n",
    "    # Negative log likelihood loss\n",
    "    nll = -tf.reduce_mean(log_likelihood)  # scalar\n",
    "\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d8a2ab0-3b83-4919-b6a0-db783a4789d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(dataset, batch_size, seq_len):\n",
    "    \"\"\"\n",
    "    dataset: list of episodes, each is a dict with keys 'z' and 'a'\n",
    "    batch_size: number of sequences per batch\n",
    "    seq_len: number of time steps (excluding the +1 for teacher forcing)\n",
    "    \n",
    "    Returns:\n",
    "        z_batch: (B, T+1, latent_size)\n",
    "        a_batch: (B, T+1, action_size)\n",
    "    \"\"\"\n",
    "    z_batch = []\n",
    "    a_batch = []\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        # Randomly pick an episode\n",
    "        episode = np.random.choice(dataset)\n",
    "        z_seq = episode['mu']\n",
    "        a_seq = episode['actions']\n",
    "\n",
    "        # Make sure it's long enough\n",
    "        assert len(z_seq) >= seq_len + 1\n",
    "\n",
    "        # Randomly choose start index\n",
    "        start = np.random.randint(0, len(z_seq) - seq_len - 1)\n",
    "\n",
    "        # Slice out a window of length seq_len + 1\n",
    "        z_chunk = z_seq[start : start + seq_len + 1]\n",
    "        a_chunk = a_seq[start : start + seq_len + 1]\n",
    "\n",
    "        z_batch.append(z_chunk)\n",
    "        a_batch.append(a_chunk)\n",
    "\n",
    "    return np.array(z_batch), np.array(a_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4eda4481-7973-4a13-94a8-9661c80baf22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_31576/3262445165.py\", line 22, in train_step  *\n        done_loss = tf.keras.losses.binary_crossentropy(done_targets, done_preds)\n\n    NameError: name 'done_targets' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m targets \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(raw_z[:, \u001b[38;5;241m1\u001b[39m:, :], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;66;03m#teacher forcing + convert to tensor type\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Learning rate decay (optional)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(min_learning_rate, learning_rate \u001b[38;5;241m*\u001b[39m (decay_rate \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step))\n",
      "File \u001b[0;32m~/.pyenv/versions/cs757-final-project/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filekk_5rbrv.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(inputs, targets)\u001b[0m\n\u001b[1;32m     11\u001b[0m     (mdn_params, done_preds, _) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(mdnrnn), (ag__\u001b[38;5;241m.\u001b[39mld(inputs),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[1;32m     12\u001b[0m     mdn_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(compute_mdn_loss), (ag__\u001b[38;5;241m.\u001b[39mld(targets), ag__\u001b[38;5;241m.\u001b[39mld(mdn_params)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 13\u001b[0m     done_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mbinary_crossentropy, (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[43mdone_targets\u001b[49m), ag__\u001b[38;5;241m.\u001b[39mld(done_preds)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(mdn_loss) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_mean, (ag__\u001b[38;5;241m.\u001b[39mld(done_loss),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     15\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(mdnrnn)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mNameError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_31576/3262445165.py\", line 22, in train_step  *\n        done_loss = tf.keras.losses.binary_crossentropy(done_targets, done_preds)\n\n    NameError: name 'done_targets' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "decay_rate = 0.9999\n",
    "min_learning_rate = 0.0001\n",
    "num_steps = 5000\n",
    "z_dim = 32\n",
    "action_dim = humanoid_actions.shape[1]\n",
    "batch_size = 32\n",
    "seq_len = 10 #has to be smaller than the shortest episode!\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Create model\n",
    "mdnrnn = MDNRNN(latent_size=z_dim, action_size=action_dim)\n",
    "\n",
    "# Custom training step\n",
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        mdn_params, done_preds, _ = mdnrnn(inputs, training=True)\n",
    "        mdn_loss = compute_mdn_loss(targets, mdn_params)\n",
    "        done_loss = tf.keras.losses.binary_crossentropy(done_targets, done_preds)\n",
    "        loss = mdn_loss + tf.reduce_mean(done_loss)\n",
    "    \n",
    "    gradients = tape.gradient(loss, mdnrnn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, mdnrnn.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "for step in range(num_steps):\n",
    "    # Simulate or load a batch of data\n",
    "    raw_z, raw_a = random_batch(episodes, batch_size, seq_len)\n",
    "\n",
    "    # inputs: [z_t, a_t] — everything except last time step\n",
    "    # targets: [z_{t+1}] — prediction target is next latent\n",
    "    inputs = tf.concat([raw_z[:, :-1, :], raw_a[:, :-1, :]], axis=-1) #returns tensor type\n",
    "    targets = tf.convert_to_tensor(raw_z[:, 1:, :], dtype=tf.float32) #teacher forcing + convert to tensor type\n",
    "\n",
    "    # Training step\n",
    "    loss = train_step(inputs, targets)\n",
    "\n",
    "    # Learning rate decay (optional)\n",
    "    lr = max(min_learning_rate, learning_rate * (decay_rate ** step))\n",
    "    optimizer.learning_rate = lr\n",
    "\n",
    "    # Logging\n",
    "    if step % 20 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss:.4f}, LR: {lr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af5fdca-80ca-4942-b456-1ed3f09630bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEXT STEP: need to have one-hot episode boundary dataset which we can use in random_batch.\n",
    "#then update with GPT's code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs757-final-project",
   "language": "python",
   "name": "cs757-final-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
