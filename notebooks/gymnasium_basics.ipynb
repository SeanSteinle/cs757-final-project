{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27d49599-860b-4f38-a6f6-b7857080bb96",
   "metadata": {},
   "source": [
    "# Gymnasium Basics\n",
    "\n",
    "This notebook walks through the basics of the gymnasium package and interface. We start by exploring some simple gyms and policies to solve them, then we take a closer look at our environment of interest, MuJoCo.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Basic Gym Environments](#Basic-Gym-Environments)\n",
    "    - [Simple Environments, Random Policies](#Simple-Environments,-Random-Policies)\n",
    "    - [Solving LunarLander with StableBaselines](#Solving-LunarLander-with-StableBaselines)\n",
    "3. [The MuJoCo Environment](#The-MuJoCo-Environment)\n",
    "    - [Simple Tasks, Random Policies](#Simple-Tasks,-Random-Policies)\n",
    "    - [Making Humans Walk with StableBaselines](#Making-Humans-Walk-with-StableBaselines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230f865b-b11c-4325-a1c7-61ee69623087",
   "metadata": {},
   "source": [
    "## Basic Gym Environments\n",
    "\n",
    "Though we're using a more complicated gymnasium environment, it's useful to get familiar with the library and its interface through simple examples. We'll explore 3-4 of these examples in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35743c7-11f7-4073-b76b-82510eda98b2",
   "metadata": {},
   "source": [
    "### Simple Environments, Random Policies\n",
    "\n",
    "To begin, let's see how agents perform in these environments whenever they're chosing random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b41cdec-e097-4f58-b1ab-615b4e4f934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "216d5586-8868-447e-b77a-50576b223870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_agent(env_name, model=None, n_timesteps=50, render_mode=\"human\"):\n",
    "    \"\"\"Plays 50 games according to a policy. Random policy is default.\"\"\"\n",
    "    env = gym.make(env_name, render_mode=render_mode)\n",
    "    obs, info = env.reset()\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(n_timesteps):  # Run for 50 timesteps or until the episode ends\n",
    "        env.render()\n",
    "        if model: #action selection -- either according to passed in policy or random policy\n",
    "            action, _ = model.predict(obs)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "    env.close()\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce342e-9e40-4909-9581-5cfdf40469ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CartPole-v1...\n",
      "Running MountainCar-v0...\n",
      "Running Acrobot-v1...\n"
     ]
    }
   ],
   "source": [
    "environments = [\"CartPole-v1\", \"MountainCar-v0\", \"Acrobot-v1\", \"LunarLander-v3\"]\n",
    "for env_name in environments:\n",
    "    print(f\"Running {env_name}...\")\n",
    "    play_agent(env_name, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c27a1-c709-457e-b592-166edc874f81",
   "metadata": {},
   "source": [
    "Performance is pretty poor! But this gives us a sense of the environment and what to expect out of optimal policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745aee4-5d3c-46dc-9633-f42f1548b2e1",
   "metadata": {},
   "source": [
    "### Solving LunarLander with StableBaselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333580d9-dfeb-4f0b-b3da-942abed0dd1e",
   "metadata": {},
   "source": [
    "Let's focus in on the [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) environment. The goal of this environment is to land a craft between two flags. You get positive reward for episodes where you land between the flags, negative reward else. We will first see an agent which uses random actions play, then an agent which we've trained, then finally we can compare their rewards over 1000 timesteps. Note it typically takes about 1M timesteps to train a decent LunarLander which may take a few minutes! Go grab a cup of coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509fc8c0-a5bf-49e6-b3b4-c1377dcd4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "def train_agent(env_name, timesteps, save_path=\"\", render_mode=None):\n",
    "    \"\"\"Train a simple PPO policy with an MLP, available simply via StableBaselines3!\"\"\"\n",
    "    # env = gym.make(env_name)\n",
    "    env = gym.make(env_name, render_mode=render_mode)\n",
    "\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    if save_path != \"\":\n",
    "        model.save(save_path)\n",
    "    env.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c7273-e9eb-42ef-8bd6-8aa187b55437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timesteps = 1000\n",
    "os.makedirs(\"../models/\", exist_ok=True)\n",
    "if f'lunar_lander_{timesteps}.zip' not in os.listdir(\"../models/basic/\"):  #training our model -- note: 1M is needed for 'good' performance, but that takes a while\n",
    "    print(f\"no model found, training new model for {timesteps} timesteps\")\n",
    "    lunar_lander_model = train_agent('LunarLander-v3', timesteps, render_mode=None) #train lunar lander model\n",
    "    lunar_lander_model.save(f\"../models/basic/lunar_lander_{timesteps}\")\n",
    "else:\n",
    "    print(f\"model found, loading cached model!\")\n",
    "    lunar_lander_model = PPO.load(f\"../models/basic/lunar_lander_{timesteps}.zip\") #load lunar lander model if cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef99f0d-04fd-41fc-afc8-d3669c6d2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 1000\n",
    "random_rewards = play_agent(\"LunarLander-v3\", None, timesteps, render_mode=None) #test + show random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbee55f4-1500-4fd0-a39d-8afb03aa1f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_rewards = play_agent(\"LunarLander-v3\", lunar_lander_model, timesteps, render_mode=None) #test + show trained policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d675ed-7a3a-4427-b779-19cade5ea54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare random and trained policy results visually!\n",
    "plt.plot(range(timesteps), random_rewards, color='red', label='Random Policy')\n",
    "plt.plot(range(timesteps), trained_rewards, color='blue', label='PPO Policy')\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Comparing Random and PPO Policies in the LunarLander Environment\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "cumulative_random = np.cumsum(random_rewards)\n",
    "cumulative_trained = np.cumsum(trained_rewards)\n",
    "\n",
    "plt.plot(range(timesteps), cumulative_random, color='red', label='Cumulative Random Policy Reward')\n",
    "plt.plot(range(timesteps), cumulative_trained, color='blue', label='Cumulative PPO Policy Reward')\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.title(\"Cumulative Reward Comparison\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average Reward (Random Policy): {np.mean(random_rewards):.2f} ± {np.std(random_rewards):.2f}\")\n",
    "print(f\"Average Reward (PPO Policy): {np.mean(trained_rewards):.2f} ± {np.std(trained_rewards):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b7850f-54a9-4fed-b24a-0fe398e9141d",
   "metadata": {},
   "source": [
    "## The MuJoCo Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484b128-2c55-4c41-8944-63c596f55ec8",
   "metadata": {},
   "source": [
    "### Simple Tasks, Random Policies\n",
    "\n",
    "To get situated within the MuJoCo environment let's explore the [InvertedPendulum](https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/) and [Reacher](https://gymnasium.farama.org/environments/mujoco/reacher/) environments! These are great environments to start with because they're simple and also fairly direct analogs for simpler gym environments, [Cartpole](https://gymnasium.farama.org/environments/classic_control/cart_pole/) and [Acrobot](https://gymnasium.farama.org/environments/classic_control/acrobot/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221ccf4-c4c2-41cf-8a6e-fe6d3207f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = play_agent('InvertedPendulum-v5', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc70dd-4531-40b7-8d84-47a3bbf94fe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = play_agent('Reacher-v5', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cdef40-e4c8-428a-b50e-be8b870e5842",
   "metadata": {},
   "source": [
    "### Making Humans Walk with StableBaselines\n",
    "\n",
    "As mentioned above, the InvertedPendulum and Reacher are really just versions of trivial classic control problems. But we're going to be using world models, so let's do something a little harder! Let's make a human walk with the [Humanoid]() environment!\n",
    "\n",
    "This environment is much more complex than the simple MuJoCo environments. It has 16 actions and 44 observations, far more than any environment we've seen. It also has a more complex reward sturcture, rewarding forward movement but also regularizing against large motions. More than just the environment structure, the simulation itself is also more complex and takes much longer to render than simple environments -- again, this makes for a great candidate for world models, where we might be able to train efficiently in dreams!\n",
    "\n",
    "Let's start by observing a random policy. Then we'll train a policy with StableBaselines and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478ebb6b-b69e-40f9-9d9c-b87c54df8cdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timesteps = 500000\n",
    "\n",
    "if f'humanoid_{timesteps}.zip' not in os.listdir(\"../models/basic/\"):\n",
    "    print(f\"no model found, training new model for {timesteps} timesteps\")\n",
    "    humanoid_model = train_agent('Humanoid-v5', timesteps, render_mode=None) #train humanoid model\n",
    "    humanoid_model.save(f\"../models/basic/humanoid_{timesteps}\")\n",
    "else:\n",
    "    print(f\"model found, loading cached model!\")\n",
    "    humanoid_model = PPO.load(f\"../models/basic/humanoid_{timesteps}.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d236adf-a557-4075-9708-a2b4fa184993",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 1000\n",
    "random_rewards = play_agent('Humanoid-v5', None, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da434b01-3610-4f62-b36f-a367b1c59a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_rewards = play_agent('Humanoid-v5', humanoid_model, timesteps) #test humanoid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa935575-977f-4c5f-9ab2-b0514363c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare random and trained policy results visually!\n",
    "plt.plot(range(timesteps), random_rewards, color='red', label='Random Policy')\n",
    "plt.plot(range(timesteps), trained_rewards, color='blue', label='PPO Policy')\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Comparing Random and PPO Policies in the Humanoid Environment\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "cumulative_random = np.cumsum(random_rewards)\n",
    "cumulative_trained = np.cumsum(trained_rewards)\n",
    "\n",
    "plt.plot(range(timesteps), cumulative_random, color='red', label='Cumulative Random Policy Reward')\n",
    "plt.plot(range(timesteps), cumulative_trained, color='blue', label='Cumulative PPO Policy Reward')\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.title(\"Cumulative Reward Comparison\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average Reward (Random Policy): {np.mean(random_rewards):.2f} ± {np.std(random_rewards):.2f}\")\n",
    "print(f\"Average Reward (PPO Policy): {np.mean(trained_rewards):.2f} ± {np.std(trained_rewards):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28b1c2-58c9-4ca1-bfea-81f12e870b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(trained_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ddf2b1-d7f2-4353-8e3e-b5daeebba1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(humanoid_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
